\chapter{Adversaries}
My goal here is to first discuss the risks to data without discussing machine learning to develop a pedgaogical approach to the ML equivalent.
\section{Computer Security}
\lipsum[1-2]
\subsection{Intrusion}
\lipsum[1-2]
\subsection{Exfiltration}
\lipsum[1-2]
\subsection{Remote Control}
\lipsum[1-2]
\subsection{Denial of Service and Spam}
\lipsum[1-2]

\section{Adversarial Attacks for Machine Learning}
\label{sec:attacks}
There are several types of attacks that target machine learning models or data at different phases. My work focuses on attacks where an attacker seeks to \emph{evade} a classifier by confusing the model at run-time (e.g. a penetrated network or a malicious application being detected as benign). Robustness---the ability to withstand attacks---is usually measured by the accuracy gap before and after being attacked (benign and adversarial accuracy, respectively).  While much work has gone into evaluating the robustness for neural networks~\cite{deepfool,biggio_evasion_2013,carlini_towards_2017} and regression algorithms~\cite{deka2019adversarial}, these model exist in the real-world, to the objections of many researchers \cite{meyers2023safety,biggio_evasion_2013,carlini_towards_2017}. 
Machine learning has proven to be useful in safety-critical applications like system intrusion detection~\cite{kim2003network}, network anomaly detection~\cite{mehmood2015svm}, and image recognition~\cite{tzotsos2008support}, aviation\cite{ai_aviation}, policing \cite{ai_prison}, security checkpoints, \cite{ai_security}, and medical imaging \cite{ai_medical_imaging}. However, many such attacks threaten the real-world operation of these systems.

\subsection{Inference}
\label{subsec:inference_attacks}
Given sufficient input/output pairs, all models are invertible. In the case of continuous functions (i.e. gradient descent), an auxiliary model can be trained and used as a discriminator to train an attack model. In the case of discontinuous optimisation functions (i.e. decision trees), a continuous auxiliary function can still be used. Any attacker can find unknown parameters or features given standard API access by querying $d+1$ random $d$ dimensional inputs for unknown $d+1$ parameters~\cite{chakraborty_adversarial_2018}. However, by randomising the model available to the attacker, we can increase the search space for the attacker. While this works for small-scales, it seems infeasible to create more than a few models by hand. Automated generation techniques must create statistically independent models or fall victim to auxiliary function attacks above~\cite{gauss_aug}. Other ensemble models should work similarly~\cite{gauss_out}, with weights assigned dynamically or otherwise having linearly independent models. Regression techniques are normally used in anomaly detection systems because they quickly separate an input space into two classes: normal and abnormal. However, any given model is very weak to attack because of the small number of trainable parameters~\cite{chakraborty_adversarial_2018}.


\subsection{Extraction}
\label{subsec:extraction_attacks}
While assumed the adversary has access to the model gradients with respect to the loss function, it can be approximated through statistical techniques or via other attacks~\cite{wang2019security,chakraborty2018adversarial}. It is not necessary to know all of the model parameters, just the weights and biases that compose the fitted model. Although even this constraint is broken by other attacks~\cite{wang2019security,chakraborty2018adversarial}. In many cases, perfect knowledge of a modelling process is provided normally by the peer-review process and published code. However, many models are proprietary and can only be accessed through an API that returns only the classification, either as a probability distribution or the classification labels alone \cite{tramer2016stealing}. In either case, stealing a model is both possible and generally cheaper than training it from scratch.

\subsection{Poisoning}
\label{subsec:poisoning_attacks}
Poisoning attacks attack the model itself during training. All possible techniques for preventing this type of attack rely on detecting adversarial examples. The most concrete example is intentionally mislabelling data such that a stop sign is labelled as anything but in an attempt to confuse the model. Mechanical Turk, CAPTCHA, and other labelling services can prevent this by getting labels from many users and quantifying each users consensus accuracy~\cite{ahn2003captcha}. When examining the ability of users to poison global ratings on the MovieLens data set, Li \textit{et al.} (2016) showed it was possible to poison even very large datasets in ways that made attackers statistically indistinguishable from real users~\cite{li_general_2016}. However, here are various sampling techniques that can be used to generate more robust datasets. Mehta \textit{et al.} showed that various sampling techniques were useful in creating models that generalised well and with high accuracy. They use a variety of random, stratified, and gaussian sampling techniques with efficacy seeming to vary by dataset rather than by model. Perhaps generating independent models from different population samples is an appropriate way to move forward.

\subsection{Evasion}
\label{subsec:evasion_attacks}
Evasion attacks aim to manipulate input data during the inference phase to deceive the model into misclassifying certain inputs or otherwise change the behavior of a modelt~\cite{ carlini_towards_2017, adversarialpatch, pixelattack, hopskipjump}. In poisoning attacks, the attacker intentionally injects malicious or manipulated training samples into the training dataset to influence the model's behaviour at run-time~\cite{biggio_poisoning_2013, saha2020hidden}. These attacks exploit the model's output or responses to obtain sensitive information about the training data or other confidential details~\cite{chakraborty_adversarial_2018, orekondy2019knockoff}.
Model Inversion Attacks: Model inversion attacks aim to infer sensitive information about the training data or proprietary model by exploiting the model's outputs~\cite{chakraborty_adversarial_2018, choquette2021label, li2021membership}. 

Misclassification attacks rely on the realization that any input not enclosed in the input space, but sufficiently far from class boundaries, will always result in a high confidence classification for the attack. Reducing the bit depth of the input space (color-depth in an image, for example) was shown to reduce the propensity of these attacks, but at the cost of accuracy. Similarly, using pooling layers or other smoothing functions on the input space reduced adversarial noise, but at the cost of model accuracy. Collectively these are known as feature squeezing~\cite{feature_squeezing}. In safety critical applications like autonomous vehicles, this reduction in accuracy is not a justifiable trade-off. 

In many cases, clustering and classification algorithms rely only on distance from a boundary condition to determine a class label for some input. However, by explicitly defining intra- as well as inter-class boundaries, this problem goes away. However, drawing envelopes around a given class is tantamount to knowing the probability distribution of the population in advance-- the exact thing that our model seeks to discover. While this is handled in in low-dimensional data through simulation and Monte Carlo methods, this does not work for high dimensional data like images or very wide data tables associated with something like Netflow data. So, we must reduce dimensionality or generate new samples. 

We can use classical machine learning techniques to do just that. Ghosh \textit{et al.} showed that Gaussian Mixture Variational Autoencoders can be used to detect adversarial samples by rejecting samples that have low probability of their class label or have especially high reconstruction error~\cite{gho}. Ghosh \textit{et al.} used Mahalanobis distance to calculate the distance between a sample at point P and a distributution D and set some explicit cut-off score to great effect. However, the authors admit that classes need to be well separated and balanced, which is rarely the case for anomaly detection systems. The autoencoder technique was shown to work well with images classification tasks, correctly rejecting 97% of adversarial examples.

Similarly, we can build a neural network to generate fake data and teach any other model to reject it. By doing this recursively, we can create a generative adversarial model that creates 'noise' that is indistinguishable from real data (to the original model). By labelling this new data, we can trust a model that is robust to this set of peturbations(Samanagouei \textit{et al.})
However, this merely creates a new hyperparameter set and starts the attack cycle with a new black box, even if it is computationally more expensive to attack. MagNet seems to be the most well-cited example that relies on a classifier and a randomly selected autoencoder (to make attacks harder). 
